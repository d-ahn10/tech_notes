---Common Runtime Complexities---

Constant Time O(1)
No matter how many elements we're working with, the algorithm/operation/whatever will always take the same amount of time.

Logarithmic Time O(log n) "search"
You have this if doubling the number of elements you are iterating over doesn't double the amount of work.
Always assume that searching operations are log(n). Example: sorting through a sorted array of data.

Linear Time O(n)
Iterating through all elements in a collection of data. If you see a for loop spanning from '0' to 'array.length', you probably have 'n', or linear runtime.

Quasilinear Time O(n log n) "sort"
You have this if doubling the number of elements you are iterating over doesn't double the amount of work.
Always assume that any any sorting operation is n*log(n).

Quadratic Time O(n^2)
Every element in a collection has to be compared to every other element. 'The handshake problem'

Exponential Time O(2^n)
If you add a *single* element to a collection, the processing power required doubles.
(if you run into this during an interview, usually it means the interviewer wants to see that you can recognize that the algorithm is inefficient and that you can suggest/implement a much more difficult to code out solution, BUT it'll run better such as Linear Runtime)

---Identifying Runtime Complexity---

Iterating with a simple for loop thorugh a single collection?
-> Probably O(n)

Iterating through half a collection?
-> Still O(n). There are no constants in runtime.

Iterating through two *different* collections with separate for loops?
-> O(n + m)

Two nested for loops iterating over the same collection?
-> O(n^2)

Two nested for loops iterating over different collection?
-> O(n*m)

Sorting?
-> O(n*log(n))

Searching a sorted array?
-> O(log(n))

---Space Complexity--- (less frequently asked)

How much more memory is required by doubling the problem set?